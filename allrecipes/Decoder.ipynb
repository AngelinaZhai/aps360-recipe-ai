{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":164,"status":"ok","timestamp":1657557975402,"user":{"displayName":"Nat S","userId":"04218884030039370618"},"user_tz":240},"id":"4z3WKcdPCjDW"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision  \n","from torchvision import datasets\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import matplotlib.pyplot as plt\n","import csv\n","import os\n","from os import listdir\n","import torch\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import sys\n","import array\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1657562953435,"user":{"displayName":"Nat S","userId":"04218884030039370618"},"user_tz":240},"id":"jQS9U-QfTO_b","outputId":"8115f0a1-fb79-4b98-f3a3-aa2e48109e0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Data Loading Done.\n","Loading Done.\n","12351\n"]}],"source":["data = [] #total cleaned data array\n","\n","# all_ingredients = set({}) #total types of ingredients data set\n","\n","# Reading AllRecipes CSV file\n","#['Recipe Name;Review Count;Recipe Photo;Author;Prepare Time;Cook Time;Total Time;Ingredients;Directions;RecipeID']\n","#   0           1               2          3        4           5           6           7         8         9\n","os.chdir(r\"C:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\")\n","with open('clean_recipes.csv', mode='r') as f:\n","    read = f.readlines()\n","    n = -1\n","    for line in read:\n","        tmp1, tmp2, tmp3 = [], [], []\n","        tmp1 += line.split(';')\n","        tmp2.append(tmp1[0]) #Recipe name\n","        tmp2.append(tmp1[2]) #Recipe photo URL\n","        tmp3 = tmp1[7].split(',') #spliced cleaned ingredients\n","        tmp2.append(tmp3)\n","        # print(tmp3)\n","        # all_ingredients.update(tmp3)\n","        # for j in range (0, len(tmp3)):\n","        #     tmp_point = tmp3[j]\n","        #     all_ingredients.update(tmp_point)\n","\n","        # all_ingredients |= set(tmp3)\n","        tmp2.append(tmp1[8].split('**')) #spliced cooking directions\n","        tmp2.append(n)\n","        data.append(tmp2) #add to total list\n","        n += 1\n","\n","del data[0] #remove label row\n","\n","\n","print(\"Original Data Loading Done.\")\n","\n","print(\"Loading Done.\")\n","print(len(data))\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1657562954348,"user":{"displayName":"Nat S","userId":"04218884030039370618"},"user_tz":240},"id":"MQWfYsHRksyV","outputId":"d4f36700-f552-4919-8dc0-95584e2e372c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data Cleaning Complete.\n","11376\n"]}],"source":["# #list with index values of invalid images\n","trashIndex = [12331, 12328, 12310, 12301, 12291, 12299, 12274, 12263, 12257, 12252, 12243, 12238, 12234, 12221, 12217, 12212, 12207, 12162, 12136, 12131, 12119, 12118, 12111, 12080, 12077, 12067, 12040, 12038, 12032, 12022, 12020, 12018, 12016, 12013, 11997, 11993, 11954, 11950, 11945, 11926, 11916, 11897, 11888, 11882, 11863, 11861, 11859, 11857, 11856, 11855, 11854, 11849, 11843, 11841, 11786, 11785, 11779, 11769, 11752, 11723, 11720, 11705, 11668, 11662, 11640, 11634, 11604, 11556, 11514,\n","11500, 11490, 11463, 11425, 11386, 11384, 11383, 11377, 11369, 11367, 11362, 11359, 11357, 11355, 11351, 11347, 11302, 11288, 11265, 11264, 11263, 11261, 11260, 11249,\n","11229, 11172, 11163, 11158, 11151, 11139, 11114, 11113, 11112, 11107, 11077, 11037, 11023, 11002, 10980, 10965, 10953, 10913, 10906, 10903, 10886, 10864, 10863, 10860, 10840, 10824, 10796, 10774, 10748, 10744, 10743, 10738, 10737, 10736, 10731, 10723, 10722, 10701, 10683, 10665, 10658, 10645, 10644, 10641, 10630, 10592, 10586, 10565, 10564, 10550, 10546, 10542, 10519, 10510, 10484, 10480, 10469, 10461, 10439, 10419, 10408, 10377, 10369, 10364, 10337, 10334, 10325, 10323, 10314, 10313, 10311, 10306, 10301, 10229, 10209,\n","10191, 10178, 10174, 10165, 10153, 10145, 10136, 10104, 10100, 10082, 10066, 10059, 10051, 10047, 9997, 9983, 9978, 9969, 9965, 9951, 9909, 9907, 9901, 9897, 9890, 9865, 9859, 9844, 9839, 9828, 9820, 9816, 9804, 9767, 9766, 9756, 9736, 9717, 9683, 9666, 9654, 9637, 9636, 9622, 9619, 9606, 9599, 9596, 9595, 9566, 9559, 9509, 9500, 9495, 9472, 9466, 9461, 9424, 9423, 9422, 9398, 9386, 9385, 9373, 9330, 9327, 9323, 9299, 9283, 9274, 9245, 9240, 9227, 9195, 9149, 9098, 9035, 9022, 8980, 8976, 8974, 8920, 8917, 8915, 8911, 8910, 8895, 8888, 8887, 8884, 8878, 8872, 8862, 8850, 8838, 8811, 8804, 8757, 8739, 8737, 8724, 8637, 8636, 8635,\n","8630, 8629, 8612, 8610, 8607, 8567, 8546, 8544, 8533, 8521, 8514, 8505, 8488, 8469, 8466, 8459, 8442, 8401, 8385, 8382, 8380, 8373, 8351, 8348, 8340, 8339, 8336, 8325, 8321, 8305, 8281, 8273, 8243, 8238, 8230, 8206, 8204, 8175, 8166, 8165, 8160, 8159, 8158, 8152, 8149, 8145, 8141, 8132, 8115, 8099, 8096, 8091, 8079, 8067, 8063, 8034, 8033, 8024, 8007, 7974, 7921, 7920, 7911, 7906, 7903, 7902, 7898, 7895, 7893, 7892, 7890, 7889, 7885, 7872, 7871, 7855, 7824, 7815, 7809, 7772, 7761, 7745, 7713, 7712, 7707, 7704, 7688, 7667, 7665, 7642, 7640, 7625, 7577, 7542, 7539, 7533, 7529, 7512, 7494, 7488, 7484, 7477, 7464, 7459, 7388, 7366, 7365, 7352, 7350, 7323, 7314, 7279, 7258,\n","7257, 7256, 7216, 7185, 7175, 7165, 7163, 7119, 7111, 7089, 7050, 6999, 6927, 6896, 6879, 6850, 6791, 6742, 6736, 6728, 6700, 6681, 6655, 6637, 6567, 6542, 6541, 6533, 6513, 6503, 6494, 6481, 6471, 6435, 6425, 6408, 6392, 6384, 6365, 6347, 6341, 6338, 6334, 6330, 6314, 6313, 6237, 6220, 6200, 6177, 6174, 6155, 6144, 6140, 6082, 6069, 5993, 5980, 5975, 5968, 5957, 5954, 5953, 5949, 5941, 5932, 5929, 5926, 5915, 5897, 5840, 5817, 5815, 5805, 5784, 5725, 5724, 5715, 5711, 5710, 5706, 5693, 5692, 5685, 5676, 5656, 5650, 5579, 5560, 5449, 5350, 5332, 5324, 5313, 5299, 5298, 5294, 5278, 5273, 5270, 5269, 5263, 5163, 5162, 5154, 5124, 5091, 5080, 8063, 5055, 5053, 5048,\n","5045, 5042, 5037, 4992, 4990, 4980, 4969, 4956, 4891, 4886, 4882, 4850, 4845, 4837, 4832, 4816, 4812, 4803, 4801, 4791, 4790, 4789, 4769, 4738, 4687, 4675, 4661, 4660, 4632, 4626, 4625, 4603, 4587, 4582, 4581, 4559, 4538, 4527, 4525, 4518, 4505, 4504, 4497, 4496, 4456, 4432, 4425, 4411, 4361, 4316, 4304, 4214, 4204, 4197, 4188, 4183, 4169, 4167, 4140, 4139, 4127, 4122, 4120, 4115, 4107, 4105, 4097, 4090, 4076, 4073, 4069, 4068, 4059, 4055, 4046, 4044, 4042, 4036, 4035, 4033, 4029, 4028, 4026, 4019, 4006, 3995, 3990, 3989, 3974, 3959, 3954, 3943, 3931, 3899, 3897, 3896, 3891, 3876, 3858, 3844, 3836, 3833, 3832, 3831, 3828, 3819, 3783, 3781, 3777, 3769, 3767, 3756, 3745, 3728, 3725,\n","3675, 3617, 3612, 3562, 3549, 3532, 3513, 3498, 3451, 3444, 3439, 3438, 3428, 3422, 3418, 3416, 3406, 3399, 3395, 3374, 3358, 3356, 3344, 3343, 3332, 3329, 3317, 3302, 3282, 3277, 3263, 3257, 3252, 3247, 3245, 3224, 3221, 3214, 3213, 3209, 3208, 3206, 3194, 3185, 3183, 3182, 3181, 3177, 3167, 3162, 3136, 3131, 3118, 3117, 3106, 3100, 3090, 3087, 3085, 3075, 3072, 3070, 3062, 3061, 3052, 3039, 3029, 3026, 3018, 3013, 3012, 2988, 2978, 2976, 2956, 2952, 2949, 2943, 2933, 2929, 2901, 2896, 2887, 2886, 2868, 2864, 2855, 2845, 2842, 2831, 2829, 2820, 2816, 2812, 2806, 2792, 2779, 2772, 2770, 2769, 2766, 2763, 2756, 2739, 2737, 2733, 2731, 2727, 2723, 2720, 2718, 2713, 2708, 2700, 2683, 2681, 2672,\n","2663, 2643, 2637, 2635, 2632, 2631, 2626, 2624, 2614, 2607, 2597, 2594, 2592, 2591, 2590, 2585, 2580, 2561, 2559, 2554, 2553, 2552, 2548, 2530, 2529, 2511, 2502, 2491, 2484, 2483, 2453, 2452, 2447, 2446, 2428, 2425, 2409, 2407, 2398, 2396, 2367, 2363, 2356, 2355, 2349, 2344, 2297, 2293, 2287, 2274, 2269, 2264, 2253, 2242, 2222, 2219, 2210, 2199, 2195, 2181, 2170, 2165, 2164, 2152, 2150, 2149, 2143, 2098, 2091, 2089, 2085, 2083, 2082, 2079, 2076, 2073, 2070, 2063, 2061, 2051, 2045, 2038, 2036, 2033, 2032, 2030, 2029, 2027, 2021, 2019, 2012, 2001, 1992, 1985, 1981, 1975, 1959, 1957, 1946, 1936, 1910, 1875, 1873, 1843, 1827, 1813, 1809, 1799, 1787, 1782, 1778, 1770, 1768, 1758, 1754,\n","1746, 1742, 1739, 1736, 1733, 1729, 1723, 1720, 1713, 1697, 1691, 1627, 1626, 1599, 1594, 1584, 1578, 1577, 1543, 1509, 1487, 1419, 1412, 1351, 1336, 1310, 1287, 1254, 1226, 1194, 1174, 1102, 1090, 1067, 1032, 1029, 1014, 990, 988, 984, 970, 961, 957, 944, 934, 928, 920, 917, 904, 870, 868, 834, 820, 807, 806, 776, 773, 772, 762, 756, 755, 752, 731, 726, 721, 718, 714, 709, 692, 688, 674, 672, 670, 669, 665, 663, 659, 652, 650, 626, 617, 612, 593, 591, 583, 574, 544, 538, 537, 523, 522, 502, 494, 479, 470, 469, 461, 456, 449, 439, 431, 430, 418, 403, 399, 389, 382, 347, 344, 342, 341, 338, 337, 301, 295, 291, 287, 274, 260, 239, 237, 236, 235, 228, 219, 210, 163, 102, 93, 78, 60]\n","\n","# # print(len(data))\n","# # print(len(trashIndex))\n","\n","\n","#deleting images and data entries at specified indices\n","for i in range (0, len(trashIndex)):\n","    del data[trashIndex[i]]\n","\n","# # print(len(data))\n","print(\"Data Cleaning Complete.\")\n","print(len(data))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":181,"status":"ok","timestamp":1657562951527,"user":{"displayName":"Nat S","userId":"04218884030039370618"},"user_tz":240},"id":"vCdo4Ph9kvZd","outputId":"5c786833-c6c7-43a7-f618-8d002e6f76b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["11374\n","11376\n"]}],"source":["import os\n","from os import listdir\n","\n","main_list = [] #main data list, organized in sequence of: image tensor, spliced ingredients list\n","#define universal image transform\n","transform = transforms.Compose([transforms.PILToTensor()])\n","# folder_dir = \"images\"\n","index = 0\n","os.chdir(r\"C:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\\images\") #replace directory here to system directory to image folder\n","\n","for i in range(0, len(data)):\n","    try:\n","        tmp = []\n","        image = Image.open(str(data[i][4])+'.jpg')\n","        tmp_tensor = transform(image)\n","        tmp.append(tmp_tensor)\n","        tmp.append(data[i][2])\n","        # tmp.append(data[index][3]) #adding cooking directions\n","        main_list.append(tmp)\n","    except:\n","        pass\n","\n","\n","# os.chdir(\"..\") #reset directory to previous layer\n","os.chdir(r\"C:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\")\n","print(len(main_list))\n","print(len(data))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":168,"status":"error","timestamp":1657560288229,"user":{"displayName":"Nat S","userId":"04218884030039370618"},"user_tz":240},"id":"FubD5TRKSpHd","outputId":"49a6ebb8-2533-4785-ad61-bd8442a235b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["11368\n"]}],"source":["# Only keeping images with the right tensor size\n","data_list = []\n","for i in range(len(main_list)):\n","  if main_list[i][0].size()==torch.Size([3, 224, 224]):\n","    data_list.append(main_list[i])\n","print(len(data_list))\n","\n","# Checking for None\n","for i in range(len(data_list)):\n","  if data_list[i]==None:\n","    print(i)\n","\n","# Dividing into training and validation data\n","split = int(6/7*len(data_list))\n","data_train = data_list[:split]\n","data_val = data_list[split:]\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":662,"status":"ok","timestamp":1657560237532,"user":{"displayName":"Nat S","userId":"04218884030039370618"},"user_tz":240},"id":"uS4nqp2SL-o5","outputId":"40ffe2c8-2420-4e5c-aba9-0a1160066018"},"outputs":[{"name":"stdout","output_type":"stream","text":["Master ingredients list loading complete.\n"]}],"source":["os.chdir(r\"C:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\")\n","from open_npz import ingredients"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        super(EncoderCNN, self).__init__()\n","        resnet = torchvision.models.resnet50(pretrained=True)\n","        #for param in resnet.parameters():\n","            #param.requires_grad_(False)\n","        \n","        modules = list(resnet.children())[:-1] #get rid of the last layer of ResNet to replace with classification later\n","        self.resnet = nn.Sequential(*modules)\n","        self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n","        self.batch= nn.BatchNorm1d(embed_size,momentum = 0.01)\n","        self.embed.weight.data.normal_(0., 0.02)\n","        self.embed.bias.data.fill_(0)\n","        \n","    def forward(self, images):\n","        features = self.resnet(images)\n","        features = features.view(features.size(0), -1)\n","        features = self.batch(self.embed(features))\n","        return features"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nclass Decoder(nn.Module):\\n    def __init__(self, embed_size, hidden_size, word_pool_size, num_layers=1):\\n        super(Decoder, self).__init__()\\n        self.dec_embedding = nn.Embedding(word_pool_size, embed_size)\\n        dec_layer = nn.TransformerDecoderLayer(embed_size, nhead = 2,dim_feedforward = hidden_size, dropout = 0.3)\\n        self.decoder = nn.TransformerDecoder(dec_layer, num_layers = num_layers)\\n\\n        self.dense = nn.Linear(embed_size, word_pool_size)\\n        self.log_softmax = nn.LogSoftmax()\\n    \\n    def forward(self, features, captions):\\n        transformer_out = self.decoder(captions,features)\\n        final_out = self.dense(transformer_out)\\n        return self.log_softmax(final_out)\\n'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["class Decoder(nn.Module):\n","    def __init__(self, embed_size, hidden_size, word_pool_size, num_layers=1):\n","        super(Decoder, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_size = hidden_size\n","        self.embed_size= embed_size\n","        self.drop_prob= 0.2\n","        self.vocabulary_size = word_pool_size\n","        self.lstm = nn.LSTM(self.embed_size, self.hidden_size , self.num_layers,batch_first=True)\n","        self.dropout = nn.Dropout(self.drop_prob)\n","        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size)\n","        # self.embed = nn.Embedding(self.embed_size, self.vocabulary_size)\n","        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n","        self.embed.weight.data.uniform_(-0.1, 0.1)\n","        self.linear.weight.data.uniform_(-0.1, 0.1)\n","        self.linear.bias.data.fill_(0)\n","    \n","    def forward(self, features, captions):\n","        # print(captions)\n","        # print(captions)\n","        print(self.vocabulary_size, self.embed_size)\n","        embeddings = self.embed(captions) #problem is here\n","        features = features.unsqueeze(1)\n","        embeddings = torch.cat((features, embeddings[:, :-1,:]), dim=1)\n","        hiddens, c = self.lstm(embeddings)\n","        outputs = self.linear(hiddens)\n","        return outputs\n","\n","\"\"\"\n","class Decoder(nn.Module):\n","    def __init__(self, embed_size, hidden_size, word_pool_size, num_layers=1):\n","        super(Decoder, self).__init__()\n","        self.dec_embedding = nn.Embedding(word_pool_size, embed_size)\n","        dec_layer = nn.TransformerDecoderLayer(embed_size, nhead = 2,dim_feedforward = hidden_size, dropout = 0.3)\n","        self.decoder = nn.TransformerDecoder(dec_layer, num_layers = num_layers)\n","\n","        self.dense = nn.Linear(embed_size, word_pool_size)\n","        self.log_softmax = nn.LogSoftmax()\n","    \n","    def forward(self, features, captions):\n","        transformer_out = self.decoder(captions,features)\n","        final_out = self.dense(transformer_out)\n","        return self.log_softmax(final_out)\n","\"\"\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Creating dictionaries for cross referencing ingredient name to specific number (assigned by index value)\n","ing_map = {}\n","for i in range(0,len(ingredients)):\n","    ing_map.update({ingredients[i] : i})\n","\n","num_map = {}\n","for i in range(0,len(ingredients)):\n","    num_map.update({i : ingredients[i]})\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Adding in missing ingredients to dicts\n","ing_map.update({\"oat\" : 3501})\n","ing_map.update({\"poppy\" : 3502})\n","ing_map.update({\"<start>\" : 3503})\n","ing_map.update({\"<end>\" : 3504})\n","ing_map.update({\"<pad>\" : 3505})\n","\n","num_map.update({3501 : \"oat\"})\n","num_map.update({3502 : \"poppy\"})\n","num_map.update({3503 : \"<start>\"})\n","num_map.update({3504 : \"<end>\"})\n","num_map.update({3505 : \"<pad>\"})\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Converting all ingredients lists in recipes to numbers using dicts\n","def convert_list_to_num(lists):\n","    ans = []\n","    ans.append(ing_map[\"<start>\"])\n","    for i in range(0,len(lists)):\n","        ans.append(ing_map[lists[i]])\n","    ans.append(ing_map[\"<end>\"])\n","\n","\n","    for i in range(len(lists) + 2, 32):\n","        ans.append(ing_map[\"<pad>\"])\n","    return ans"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["'\\nmax_value = 0\\n\\nfor i in range(0,len(data_train)):\\n    if (len(data_train[i][1]) > max_value):\\n        max_value = len(data_train[i][1])\\n\\nprint(max_value)\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","max_value = 0\n","\n","for i in range(0,len(data_train)):\n","    if (len(data_train[i][1]) > max_value):\n","        max_value = len(data_train[i][1])\n","\n","print(max_value)\n","\"\"\""]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def train(encoder, decoder, num_epochs,batch_size,criterion,optimizer,word_pool_size):\n","    iters, losses = [],[]\n","\n","    for epoch in range(0, num_epochs):\n","\n","            images = []\n","            ing_list = []\n","\n","            for i in range(0,batch_size):\n","                try: \n","                    images.append(data_train[i + epoch*batch_size][0])\n","                    temp = convert_list_to_num(data_train[i+ epoch*batch_size][1])\n","                    temp = torch.IntTensor(temp)\n","                    ing_list.append(temp)\n","                \n","                except:\n","                    continue\n","            \n","            print(\"Length of ing_list: {}\".format(len(ing_list)))\n","\n","            if (len(ing_list) > 0):\n","                print(epoch)\n","                images = torch.stack(images)\n","                images = images.reshape([len(images), 3, 224, 224])\n","                images = images.type(torch.FloatTensor)\n","\n","                ing_list = torch.stack(ing_list)\n","                ing_list = ing_list.type(torch.IntTensor)\n","\n","                # Zero the gradients.\n","                decoder.zero_grad()\n","                encoder.zero_grad()\n","                \n","                # try:\n","                #     # pass inputs into encoder and decoder\n","                #     features = encoder(images)\n","                #     outputs = decoder(features, ing_list)\n","                #     print(outputs.shape)\n","                # except:\n","                #     # print(\"poo\")\n","                #     continue\n","\n","                # pass inputs into encoder and decoder\n","                features = encoder(images)\n","                # print(features[0])\n","                outputs = decoder(features, ing_list)\n","                print(outputs.shape)\n","                \n","                # Calculate the batch loss.\n","        #         print(\"outputs.shape: \", outputs.shape)\n","                loss = criterion(outputs.view(-1, word_pool_size), ing_list.view(-1).long())\n","                # Backward pass.\n","                loss.backward()\n","                \n","                # Update the parameters in the optimizer.\n","                optimizer.step()\n","                optimizer.zero_grad() \n","\n","                iters.append(epoch)\n","                losses.append(float(loss)/batch_size)\n","                \n","\n","    plt.title(\"Training Curve\")\n","    plt.plot(iters, losses, label=\"Train\")\n","    plt.xlabel(\"Iterations\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3505\n"]}],"source":["word_pool_size = len(ing_map)\n","print(word_pool_size)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Length of ing_list: 32\n","0\n","3505 256\n"]},{"ename":"IndexError","evalue":"index out of range in self","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\\Decoder.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=20'>21</a>\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(decoder\u001b[39m.\u001b[39mparameters()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(encoder\u001b[39m.\u001b[39membed\u001b[39m.\u001b[39mparameters()) \u001b[39m#+ list(encoder.bn.parameters())\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=21'>22</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params, lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\u001b[39m#, betas=(0.9, 0.999), eps=1e-08)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=23'>24</a>\u001b[0m train(encoder,decoder,num_epochs,batch_size,criterion,optimizer,word_pool_size)\n","\u001b[1;32mc:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\\Decoder.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(encoder, decoder, num_epochs, batch_size, criterion, optimizer, word_pool_size)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=43'>44</a>\u001b[0m         features \u001b[39m=\u001b[39m encoder(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=44'>45</a>\u001b[0m         \u001b[39m# print(features[0])\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=45'>46</a>\u001b[0m         outputs \u001b[39m=\u001b[39m decoder(features, ing_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=46'>47</a>\u001b[0m         \u001b[39mprint\u001b[39m(outputs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=48'>49</a>\u001b[0m         \u001b[39m# Calculate the batch loss.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=49'>50</a>\u001b[0m \u001b[39m#         print(\"outputs.shape: \", outputs.shape)\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\Users\\angel\\OneDrive\\Desktop\\Github Repositories\\aps360-recipe-ai\\allrecipes\\Decoder.ipynb Cell 15\u001b[0m in \u001b[0;36mDecoder.forward\u001b[1;34m(self, features, captions)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, features, captions):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=18'>19</a>\u001b[0m     \u001b[39m# print(captions)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=19'>20</a>\u001b[0m     \u001b[39m# print(captions)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocabulary_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=21'>22</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(captions) \u001b[39m#problem is here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=22'>23</a>\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/angel/OneDrive/Desktop/Github%20Repositories/aps360-recipe-ai/allrecipes/Decoder.ipynb#ch0000014?line=23'>24</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((features, embeddings[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,:]), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n","File \u001b[1;32mc:\\Users\\angel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n","\u001b[1;31mIndexError\u001b[0m: index out of range in self"]}],"source":["# Defining Hyperparameters\n","batch_size = 32 \n","embed_size = 256\n","hidden_size = 256\n","num_epochs = 3\n","\n","# The size of the vocabulary.\n","word_pool_size = len(ing_map)\n","\n","# Initialize the encoder and decoder. \n","encoder = EncoderCNN(embed_size)\n","decoder = Decoder(embed_size, hidden_size, word_pool_size)\n","\n","# Use GPUs if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","encoder.to(device)\n","decoder.to(device)\n","\n","\n","criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n","params = list(decoder.parameters()) + list(encoder.embed.parameters()) #+ list(encoder.bn.parameters())\n","optimizer = torch.optim.Adam(params, lr=0.001)#, betas=(0.9, 0.999), eps=1e-08)\n","\n","train(encoder,decoder,num_epochs,batch_size,criterion,optimizer,word_pool_size)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Decoder.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3.10.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"},"vscode":{"interpreter":{"hash":"91555e203e11ede8496fb03363e909d37d72bb051c20872b508296bfc889c327"}}},"nbformat":4,"nbformat_minor":0}
